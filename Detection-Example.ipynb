{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "from src.yolo3.model import *\n",
    "from src.yolo3.detect import *\n",
    "\n",
    "from src.utils.image import *\n",
    "from src.utils.datagen import *\n",
    "from src.utils.fixes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model():\n",
    "    '''\n",
    "    Prepare the YOLO model\n",
    "    '''\n",
    "    global input_shape, class_names, anchor_boxes, num_classes, num_anchors, model\n",
    "\n",
    "    # shape (height, width) of the imput image\n",
    "    input_shape = (416, 416)\n",
    "\n",
    "    # class names\n",
    "    class_names  = ['W','WH','WV','WHV']\n",
    "    anchor_boxes = np.array(\n",
    "        [\n",
    "        np.array([[ 73, 158], [128, 209], [224, 246]]) /32, # output-1 anchor boxes\n",
    "        np.array([[ 32,  50], [ 40, 104], [ 76,  73]]) /16, # output-2 anchor boxes\n",
    "        np.array([[ 6,   11], [ 11,  23], [ 19,  36]]) /8   # output-3 anchor boxes\n",
    "        ],\n",
    "        dtype='float64'\n",
    "    )\n",
    "\n",
    "    # number of classes and number of anchors\n",
    "    num_classes = len(class_names)\n",
    "    num_anchors = anchor_boxes.shape[0] * anchor_boxes.shape[1]\n",
    "\n",
    "    # input and output\n",
    "    input_tensor = Input( shape=(input_shape[0], input_shape[1], 3) ) # input\n",
    "    num_out_filters = ( num_anchors//3 ) * ( 5 + num_classes )        # output\n",
    "\n",
    "    # build the model\n",
    "    model = yolo_body(input_tensor, num_out_filters)\n",
    "\n",
    "    # load weights\n",
    "    weight_path = f'model-data/weights/pictor-ppe-v302-a2-yolo-v3-weights.h5'\n",
    "    print(os.getcwd())\n",
    "    print(weight_path)\n",
    "    model.load_weights( weight_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detection(img):\n",
    "    # save a copy of the img\n",
    "    act_img = img.copy()\n",
    "\n",
    "    # shape of the image\n",
    "    ih, iw = act_img.shape[:2]\n",
    "\n",
    "    # preprocess the image\n",
    "    img = letterbox_image(img, input_shape)\n",
    "    img = np.expand_dims(img, 0)\n",
    "    image_data = np.array(img) / 255.\n",
    "\n",
    "    # raw prediction from yolo model\n",
    "    prediction = model.predict(image_data)\n",
    "\n",
    "    # process the raw prediction to get the bounding boxes\n",
    "    boxes = detection(\n",
    "        prediction,\n",
    "        anchor_boxes,\n",
    "        num_classes,\n",
    "        image_shape = (ih, iw),\n",
    "        input_shape = (416,416),\n",
    "        max_boxes = 10,\n",
    "        score_threshold=0.3,\n",
    "        iou_threshold=0.45,\n",
    "        classes_can_overlap=False)\n",
    "\n",
    "    # convert tensor to numpy\n",
    "    boxes = boxes[0].numpy()\n",
    "\n",
    "    # draw the detection on the actual image\n",
    "    return draw_detection(act_img, boxes, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_imshow(img):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/akash/Documents/learning/product_Yolov5/YoloFaceV5Cropped/pictor_ppe\n",
      "model-data/weights/pictor-ppe-v302-a2-yolo-v3-weights.h5\n"
     ]
    }
   ],
   "source": [
    "# prepare the model\n",
    "prepare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(img):\n",
    "    # read the image\n",
    "    # img = cv2.imread(f'/home/akash/Documents/learning/product_Yolov5/YoloFaceV5Cropped/data/images/test.jpg')\n",
    "\n",
    "    # resize\n",
    "    img = letterbox_image(img, input_shape)\n",
    "\n",
    "    # get the detection on the image\n",
    "    img = get_detection(img)\n",
    "\n",
    "    # show the image\n",
    "    plt.imshow(img[:, :, ::-1])\n",
    "    # cv2.waitKey(5000)\n",
    "    # cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(f'/home/akash/Documents/learning/product_Yolov5/YoloFaceV5Cropped/data/images/test.jpg',1)\n",
    "test(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('testv5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "a21e0f982a8443b06969d9f1c4d10808fdf3067496994fa7c74c02339fdb467e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
